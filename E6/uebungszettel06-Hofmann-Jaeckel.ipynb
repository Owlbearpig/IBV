{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>√úbungszettel 6</center></h1>\n",
    "<h2><center>Inhaltsbasierte Bild- und Videoanalyse - SoSe 19</center></h2>\n",
    "<h3><center>Abgabe: Mo. 24.06.2019 - 12:00 Uhr</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 - Parameter Updates (1,5 + 1,5 + 3 Punkte)\n",
    "\n",
    "#### a)\tNennen Sie 3 Probleme, die bei der Optimierung mit dem Standard-Gradientenabstiegsverfahren auftreten k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* **Falsche Lernraten**: Zu kleine Lernraten f√ºhren zu einem sehr langsamen Trainingsprozess, w√§hrend zu gro√üe Lernraten den Gradienten leicht √ºber das Ziel hinausschie√üen lassen.\n",
    "* **Lokale Minima/ Sattelpunkte**: Es kann vorkommen, dass das Gradientenabstiegsverfahren bei lokalen Minima oder Sattelpunkten zum Stillstand kommt, da die Ableitung dort Null ist. \n",
    "* **Jittering**: Wenn sich der Loss sehr schnell in einer Dimension ver√§ndert, w√§hrend die √Ñnderung in einer anderen Dimension relativ gering bleibt, kommt es vor dass der Gradient um den eigentlich optimaleren Weg herum \"jitterd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Erl√§utern Sie die Unterschiede zwischen dem Momentum-Verfahren und dem Nesterov Momentum-Verfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Beim Momentum-Verfahren wird zum zum Zeitpunkt $ t $ der n√§chste Wert $ x_{t+1} $ sowohl aus dem Gradient $ ùõªf(x_t) $ berechnet als aus der Velocity $ v_t $. \n",
    "$$ x_{t+1} = x_t + (\\mu v_t - \\alpha ùõªf(x_t)) $$ \n",
    "Beim Nestorov Verfahren wird vor der Berechnung des Gradienten die Velocity $ v_t $ zu $ x_t $ addiert.\n",
    "$$ x_{t+1} = x_t + (\\rho v_t - \\alpha ùõªf(x_t + \\rho v_t)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Das Parameterupdate berechnet sich beim Nesterov-Verfahren wie folgt (siehe Vorlesungsfolien):\n",
    "<br>\n",
    "<center> $\\nu_{t+1}=\\rho \\nu_{t}-\\alpha \\triangledown f(x_t + \\rho \\nu_t)$ </center>\n",
    "   \n",
    "<center> $x_{t+1}=x_t + \\nu_{t+1}$ </center> \n",
    "   \n",
    "> #### Um die Berechnung des Momentums zu vereinfachen, wird eine Variablenersetzung mit  $\\tilde{x}_{t}=x_t + \\rho \\nu_t$    durchgef√ºhrt. Daraus ergibt sich f√ºr die Berechnung des Momentums: \n",
    "\n",
    "<center> $\\nu_{t+1}=\\rho \\nu_{t}-\\alpha \\triangledown f(\\tilde{x}_t)$ </center> \n",
    "\n",
    "> ####    Zeigen Sie, dass f√ºr das Parameterupdate gilt:\n",
    "\n",
    "<center> $\\tilde{x}_{t+1}=\\tilde{x}_t - \\rho \\nu_t + (1 + \\rho)\\nu_{t+1}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nu_{t+1}=\\rho \\nu_{t}-\\alpha \\triangledown f(\\tilde{x}_t)$$\n",
    "\n",
    "$$x_{t+1}=x_t + \\nu_{t+1}$$\n",
    "\n",
    "$$\\tilde{x}_{t+1}=x_{t+1} + \\rho \\nu_{t+1}$$\n",
    "$$\\tilde{x}_{t+1}=x_{t} + \\nu_{t+1} + \\rho \\nu_{t+1}$$\n",
    "$$\\tilde{x}_{t+1}=x_{t} + \\rho \\nu_{t}-\\alpha \\triangledown f(\\tilde{x}_t) + \\rho \\nu_{t+1}$$\n",
    "$$\\tilde{x}_{t+1}=\\tilde{x}_{t}-\\alpha \\triangledown f(\\tilde{x}_t) + \\rho \\nu_{t+1}$$\n",
    "$$\\tilde{x}_{t+1}=\\tilde{x}_{t}-\\alpha \\triangledown f(\\tilde{x}_t) + \\rho \\nu_{t+1} - \\rho \\nu_{t} + \\alpha \\triangledown f(\\tilde{x}_t) + \\nu_{t+1} $$\n",
    "$$\\tilde{x}_{t+1}=\\tilde{x}_{t} + \\rho \\nu_{t+1} - \\rho \\nu_{t} + \\nu_{t+1} $$\n",
    "$$\\tilde{x}_{t+1}=\\tilde{x}_{t}  - \\rho \\nu_{t} + (1+\\rho) \\nu_{t+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Aufgabe 2 - Dropout Regularisierung (6 Punkte)\n",
    "#### b)\tBetrachten Sie ein neuronales Netz mit ReLU Aktivierung dessen Gewichte (und damit die Struktur) gegeben ist durch:\n",
    "\n",
    "<br> \n",
    "<center> $W_1=\\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\hspace{2cm} W_2=\\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\hspace{2cm} W_3=\\begin{pmatrix} 1 & -1 \\end{pmatrix}$ </center>\n",
    "\n",
    "<br> <center> und </center> <br>\n",
    "\n",
    "<center> $b_1=\\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\hspace{3cm} b_2=\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\hspace{4cm} b_3=1$ </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Berechnen Sie die Ausgaben f√ºr die Eingabe $x=\\left(\\begin{array}{c}1\\\\ 1\\end{array}\\right)$ zweimal, indem die folgenden Dropout-Masken verwendet werden ($\\mu_0$ bezieht sich auf die Eingabeschicht, $\\mu_1$ auf den Output der ersten Aktivierungsschicht, usw.):\n",
    "\n",
    "<br> \n",
    "<center> $\\mu_0=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\hspace{3cm} \\mu_1=\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\hspace{4cm} \\mu_2=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ </center>\n",
    "\n",
    "<br> <center> und </center> <br>\n",
    "\n",
    "<center> $\\mu_0=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\hspace{3cm} \\mu_1=\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\hspace{4cm} \\mu_2=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#################################### Erste Dropout-Masken #############################\n",
    "\n",
    "1.\n",
    "\n",
    "$x_1 = \\mu_0*x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_1 \\cdot x_1 + b_1 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} \\right ] = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$\n",
    "\n",
    "2.\n",
    "\n",
    "$x_2 = \\mu_1*\\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_2 \\cdot x_2 + b_2 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} -2 \\\\ 6 \\end{pmatrix} \\right ] = \\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix}$\n",
    "\n",
    "3.\n",
    "\n",
    "$x_3 = \\mu_2*\\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}*\\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $\n",
    "\n",
    "$ ReLU \\left[ W_3 \\cdot x_3 + b_3 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\right] = ReLU \\left[ 1 \\right ] = 1$\n",
    "\n",
    "################################# Zweite Dropout-Masken ################################\n",
    "\n",
    "1.\n",
    "\n",
    "$x_1 = \\mu_0*x = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_1 \\cdot x_1 + b_1 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right ] = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "2.\n",
    "\n",
    "$x_2 = \\mu_1*\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_2 \\cdot x_2 + b_2 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix} \\right ] = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix}$\n",
    "\n",
    "3.\n",
    "\n",
    "$x_3 = \\mu_2*\\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} $\n",
    "\n",
    "$ ReLU \\left[ W_3 \\cdot x_3 + b_3 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} + 1 \\right] = ReLU \\left[ 5 \\right ] = 5$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
