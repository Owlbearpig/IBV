{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Übungszettel 6</center></h1>\n",
    "<h2><center>Inhaltsbasierte Bild- und Videoanalyse - SoSe 19</center></h2>\n",
    "<h3><center>Abgabe: Mo. 24.06.2019 - 12:00 Uhr</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 - Parameter Updates (1,5 + 1,5 + 3 Punkte)\n",
    "\n",
    "#### a)\tNennen Sie 3 Probleme, die bei der Optimierung mit dem Standard-Gradientenabstiegsverfahren auftreten können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Erläutern Sie die Unterschiede zwischen dem Momentum-Verfahren und dem Nesterov Momentum-Verfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Das Parameterupdate berechnet sich beim Nesterov-Verfahren wie folgt (siehe Vorlesungsfolien):\n",
    "<br>\n",
    "<center> $\\nu_{t+1}=\\rho \\nu_{t}-\\alpha \\triangledown f(x_t + \\rho \\nu_t)$ </center>\n",
    "   \n",
    "<center> $x_{t+1}=x_t + \\nu_{t+1}$ </center> \n",
    "   \n",
    "> #### Um die Berechnung des Momentums zu vereinfachen, wird eine Variablenersetzung mit  $\\tilde{x}_{t}=x_t + \\rho \\nu_t$    durchgeführt. Daraus ergibt sich für die Berechnung des Momentums: \n",
    "\n",
    "<center> $\\nu_{t+1}=\\rho \\nu_{t}-\\alpha \\triangledown f(\\tilde{x}_t)$ </center> \n",
    "\n",
    "> ####    Zeigen Sie, dass für das Parameterupdate gilt:\n",
    "\n",
    "<center> $\\tilde{x}_{t+1}=\\tilde{x}_t - \\rho \\nu_t + (1 + \\rho)\\nu_{t+1}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Aufgabe 2 - Dropout Regularisierung (6 Punkte)\n",
    "#### b)\tBetrachten Sie ein neuronales Netz mit ReLU Aktivierung dessen Gewichte (und damit die Struktur) gegeben ist durch:\n",
    "\n",
    "<br> \n",
    "<center> $W_1=\\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\hspace{2cm} W_2=\\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\hspace{2cm} W_3=\\begin{pmatrix} 1 & -1 \\end{pmatrix}$ </center>\n",
    "\n",
    "<br> <center> und </center> <br>\n",
    "\n",
    "<center> $b_1=\\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\hspace{3cm} b_2=\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\hspace{4cm} b_3=1$ </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Berechnen Sie die Ausgaben für die Eingabe $x=\\left(\\begin{array}{c}1\\\\ 1\\end{array}\\right)$ zweimal, indem die folgenden Dropout-Masken verwendet werden ($\\mu_0$ bezieht sich auf die Eingabeschicht, $\\mu_1$ auf den Output der ersten Aktivierungsschicht, usw.):\n",
    "\n",
    "<br> \n",
    "<center> $\\mu_0=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\hspace{3cm} \\mu_1=\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\hspace{4cm} \\mu_2=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ </center>\n",
    "\n",
    "<br> <center> und </center> <br>\n",
    "\n",
    "<center> $\\mu_0=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\hspace{3cm} \\mu_1=\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\hspace{4cm} \\mu_2=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ </center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#################################### 1st set of masks #############################\n",
    "\n",
    "1.\n",
    "\n",
    "$x_1 = \\mu_0*x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_1 \\cdot x_1 + b_1 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} \\right ] = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$\n",
    "\n",
    "2.\n",
    "\n",
    "$x_2 = \\mu_1*\\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_2 \\cdot x_2 + b_2 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} -2 \\\\ 6 \\end{pmatrix} \\right ] = \\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix}$\n",
    "\n",
    "3.\n",
    "\n",
    "$x_3 = \\mu_2*\\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}*\\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $\n",
    "\n",
    "$ ReLU \\left[ W_3 \\cdot x_3 + b_3 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\right] = ReLU \\left[ 1 \\right ] = 1$\n",
    "\n",
    "################################# 2nd set of masks ################################\n",
    "\n",
    "1.\n",
    "\n",
    "$x_1 = \\mu_0*x = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_1 \\cdot x_1 + b_1 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 \\\\ -1 & 1 \\\\ 2 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\right ] = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "2.\n",
    "\n",
    "$x_2 = \\mu_1*\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$ ReLU \\left[ W_2 \\cdot x_2 + b_2 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & 2 & -2\\\\ 2 & -1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right] = ReLU \\left[ \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix} \\right ] = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix}$\n",
    "\n",
    "3.\n",
    "\n",
    "$x_3 = \\mu_2*\\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}*\\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} $\n",
    "\n",
    "$ ReLU \\left[ W_3 \\cdot x_3 + b_3 \\right] = ReLU \\left[ \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\cdot \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} + 1 \\right] = ReLU \\left[ 5 \\right ] = 5$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
